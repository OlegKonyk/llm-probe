# Documentation

Welcome to the llm-probe documentation. This guide provides comprehensive information about the project, its architecture, and how to work with it.

## Quick Navigation

### Getting Started
- [Getting Started Guide](getting-started.md) - How to run the application and tests

### Technical Documentation
- [Tech Stack](tech-stack.md) - Technologies, frameworks, and tools used
- [Architecture](architecture.md) - System design and component overview
- [Testing Strategy](testing-strategy.md) - Testing approaches and methodologies

## Additional Resources

- [Golden Dataset Documentation](../golden-dataset/README.md) - Test data and reference samples
- [TypeScript Testing Framework](../ts-test/README.md) - TS testing implementation details
- [Python Testing Framework](../py-test/README.md) - Python testing implementation details
- [Infrastructure Documentation](../infrastructure/README.md) - AWS deployment guide

## Project Overview

llm-probe is a comprehensive testing framework for LLM-powered applications, specifically designed to test call summarization services. It demonstrates production-ready testing practices for AI systems that produce non-deterministic outputs.

### Key Features

- Multi-language testing frameworks (TypeScript and Python)
- 8 types of testing coverage (unit, integration, component, e2e, security, performance, regression, property-based)
- Golden dataset for regression testing
- Real-time LLM integration with Ollama
- Production-ready monitoring and metrics
- AWS deployment infrastructure

## Contributing

For information about contributing to this project, please see the individual component READMEs or reach out to the maintainers.
